# agent_core/conversational_planner.py (vers√£o com corre√ß√£o do in√≠cio da conversa)

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import json

class ConversationalPlanner:
    def __init__(self, api_key: str):
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=api_key, temperature=0.7)
        self.history = []

    def start_conversation(self, topics_summary: str, schedule_summary: str):
        """ Inicia a conversa com uma sauda√ß√£o e um resumo inteligente. """
        

        # 1. Define o papel do agente 
        system_prompt = """
        Voc√™ √© um assistente de estudos inteligente e amig√°vel. Sua tarefa √© conversar com o usu√°rio para criar um cronograma de estudos personalizado. 
        Seja proativo, use os dados fornecidos para fazer sugest√µes inteligentes e guie o usu√°rio at√© um plano final.
        """
        
        # 2. Cria a primeira "pergunta" do usu√°rio, que cont√©m os dados e a instru√ß√£o para come√ßar.
        initial_user_prompt = f"""
        Ol√°! Por favor, inicie a conversa. Aqui est√£o os dados que voc√™ precisa para come√ßar:

        **Resumo dos T√≥picos das Provas que analisei:**
        {topics_summary}

        **An√°lise da sua Agenda:**
        {schedule_summary}

        Com base nisso, inicie a conversa com uma sauda√ß√£o amig√°vel, um resumo r√°pido do que voc√™ descobriu e uma pergunta aberta para come√ßarmos a planejar.
        """
        
        # 3. O hist√≥rico agora come√ßa com o papel do sistema E a primeira mensagem do "usu√°rio".
        self.history = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=initial_user_prompt)
        ]
        
        # 4. A primeira invoca√ß√£o agora tem um conte√∫do claro para a IA responder.
        initial_response = self.llm.invoke(self.history)
        self.history.append(initial_response)
        
        print("\n--- ü§ñ Assistente de Estudos ---")
        print(initial_response.content)

    def chat(self, user_input: str):
        """ Continua a conversa com a entrada do usu√°rio. """
        self.history.append(HumanMessage(content=user_input))
        
        response = self.llm.invoke(self.history)
        self.history.append(response)
        
        print(f"\n--- ü§ñ Assistente de Estudos ---\n{response.content}")

    def is_plan_finalized(self) -> dict | None:
        """
        Verifica se a conversa chegou a um plano final. Se sim, extrai as prefer√™ncias.
        """
        prompt = """
        Analise o hist√≥rico da conversa. O usu√°rio concordou em finalizar e agendar o plano?
        Se sim, extraia as seguintes informa√ß√µes em um formato JSON:
        - topics_per_day (int)
        - study_time (string, formato "HH:MM")
        - study_days (lista de inteiros, segunda=0, domingo=6)
        - priorities (lista de strings com nomes de mat√©rias/assuntos, pode ser vazia)
        - start_date (string, formato "YYYY-MM-DD")
        
        Se o plano ainda n√£o estiver finalizado, retorne um JSON com a chave "finalized" como false.
        Responda APENAS com o JSON. N√£o adicione nenhum outro texto ou formata√ß√£o.
        """
        
        finalization_check_history = self.history + [HumanMessage(content=prompt)]
        
        try:
            response = self.llm.invoke(finalization_check_history)
            cleaned_response = response.content.strip().replace('```json', '').replace('```', '')
            data = json.loads(cleaned_response)
            
            if data.get("finalized") is False:
                return None
            
            if all(k in data for k in ['topics_per_day', 'study_time', 'study_days', 'start_date']):
                # Garante que 'priorities' exista, mesmo que seja uma lista vazia
                data.setdefault('priorities', [])
                return data
            return None
        except (json.JSONDecodeError, TypeError, AttributeError):
            return None